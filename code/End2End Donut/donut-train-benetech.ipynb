{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# End-to-End Image-to-Text Model: Donut (Document Understanding Transformer) üç©\n\nDonut is a combination of a swin encoder and a bart decoder. This means that it can generate text based on an image without needing any OCR. In this notebook, I show how you can use it to train a model for this competition. This is only the starting point, so I'm sure there is much room for improvement. Still, it is so cool that this one model can do everything necessary for this competition!\n\n![donut diagram](https://raw.githubusercontent.com/clovaai/donut/master/misc/overview.png)","metadata":{}},{"cell_type":"code","source":"# You will have to restart the notebook after running this\n!pip install -U datasets transformers pyarrow polyleven -q","metadata":{"execution":{"iopub.status.busy":"2023-06-08T15:33:41.637677Z","iopub.execute_input":"2023-06-08T15:33:41.638610Z","iopub.status.idle":"2023-06-08T15:34:22.751181Z","shell.execute_reply.started":"2023-06-08T15:33:41.638567Z","shell.execute_reply":"2023-06-08T15:34:22.749898Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntfx-bsl 1.12.0 requires google-api-python-client<2,>=1.7.11, but you have google-api-python-client 2.79.0 which is incompatible.\ntfx-bsl 1.12.0 requires pyarrow<7,>=6, but you have pyarrow 12.0.0 which is incompatible.\ntensorflow-transform 1.12.0 requires pyarrow<7,>=6, but you have pyarrow 12.0.0 which is incompatible.\napache-beam 2.44.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.6 which is incompatible.\napache-beam 2.44.0 requires pyarrow<10.0.0,>=0.15.1, but you have pyarrow 12.0.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"import os\nimport re\nimport json\nfrom collections import Counter\nfrom itertools import chain\nfrom pathlib import Path\nfrom typing import List, Dict, Union, Tuple, Any\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport torch\nfrom PIL import Image\nfrom sklearn.model_selection import StratifiedKFold\nfrom torch.utils.data import DataLoader\nfrom transformers import (\n    DonutProcessor,\n    VisionEncoderDecoderConfig,\n    VisionEncoderDecoderModel,\n    get_scheduler\n)\nfrom datasets import Dataset\nfrom datasets import Image as ds_img\nfrom polyleven import levenshtein # a faster version of levenshtein","metadata":{"execution":{"iopub.status.busy":"2023-06-08T15:34:39.430455Z","iopub.execute_input":"2023-06-08T15:34:39.431097Z","iopub.status.idle":"2023-06-08T15:34:51.353767Z","shell.execute_reply.started":"2023-06-08T15:34:39.431050Z","shell.execute_reply":"2023-06-08T15:34:51.352326Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"data_dir = Path(\"/kaggle/input/benetech-making-graphs-accessible/train\")\n\nimages_path = data_dir / \"images\"\ntrain_json_files = list((data_dir / \"annotations\").glob(\"*.json\"))\n\n\nclass CFG:\n\n    # General\n    debug = False\n    num_proc = 2\n    num_workers = 2\n    gpus = 2\n\n    # Data\n    max_length = 1024\n    image_height = 560\n    image_width = 560\n\n    # Training\n    epochs = 6\n    val_check_interval = 1.0  # how many times we want to validate during an epoch\n    check_val_every_n_epoch = 1\n    gradient_clip_val = 1.0\n    lr = 3e-5\n    lr_scheduler_type = \"cosine\"\n    num_warmup_steps = 100\n    seed = 42\n    warmup_steps = 300  \n    output_path = \"output\"\n    log_steps = 200\n    batch_size = 2\n    use_wandb = True","metadata":{"execution":{"iopub.status.busy":"2023-06-08T15:34:57.194542Z","iopub.execute_input":"2023-06-08T15:34:57.195482Z","iopub.status.idle":"2023-06-08T15:34:58.381659Z","shell.execute_reply.started":"2023-06-08T15:34:57.195434Z","shell.execute_reply":"2023-06-08T15:34:58.380530Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"# First step is to transform the given annotations into a format the model can work with\n\nSince the predictions need to be in the form x1;x2;x3 and y1;y2;y3, let's make that the format we will generate. We also need to add special tokens to predict the chart type and separators to distinguish what is what.\n\nRemember, Donut takes an image as input and will generate text as output.","metadata":{}},{"cell_type":"code","source":"# Let's add chart types as special tokens and a special BOS token\n\nPROMPT_TOKEN = \"<|PROMPT|>\"\nX_START = \"<x_start>\"\nX_END = \"<x_end>\"\nY_START = \"<y_start>\"\nY_END = \"<y_end>\"\n\n\nSEPARATOR_TOKENS = [\n    PROMPT_TOKEN,\n    X_START,\n    X_END,\n    Y_START,\n    Y_END,\n]\n\nLINE_TOKEN =  \"<line>\" \nVERTICAL_BAR_TOKEN = \"<vertical_bar>\"\n# HORIZONTAL_BAR_TOKEN = \"<horizontal_bar>\"\nSCATTER_TOKEN = \"<scatter>\"\nDOT_TOKEN = \"<dot>\"\n\nCHART_TYPE_TOKENS = [\n    LINE_TOKEN,\n    VERTICAL_BAR_TOKEN,\n#     HORIZONTAL_BAR_TOKEN,\n    SCATTER_TOKEN,\n    DOT_TOKEN,\n]\n\nnew_tokens = SEPARATOR_TOKENS + CHART_TYPE_TOKENS\n\ndef round_float(value: Union[int, float, str]) -> Union[str, float]:\n    \"\"\"\n    Convert a float value to a string with the specified number of decimal places. \n    If there is more than 1 digit in the integer, then we will truncate to 1 decimal.\n    Otherwise, will truncate to 4 decimals.\n\n    Args:\n        value (int, float, str): The float value to convert\n\n    Returns:\n        str: The rounded float value as a string\n    \"\"\"\n    if isinstance(value, float):\n        value = str(value)\n\n        if \".\" in value:\n            integer, decimal = value.split(\".\")\n            if abs(float(integer)) > 1:\n                decimal = decimal[:1]\n            else:\n                decimal = decimal[:4]\n\n            value = integer + \".\" + decimal\n    return value\n\n\ndef is_nan(value: Union[int, float, str]) -> bool:\n    \"\"\"\n    Check if a value is NaN (not a number).\n\n    Args:\n        value (int, float, str): The value to check\n\n    Returns:\n        bool: True if the value is NaN, False otherwise\n    \"\"\"\n    return isinstance(value, float) and str(value) == \"nan\"\n\n\ndef get_gt_string_and_xy(filepath: Union[str, os.PathLike]) -> Dict[str, str]:\n    \"\"\"\n    Get the ground truth string and x-y data from the given JSON file.\n\n    Args:\n        filepath (str): The path to the JSON file\n\n    Returns:\n        dict: A dictionary containing the ground truth string, x-y data, chart type, id, and source\n    \"\"\"\n    filepath = Path(filepath)\n\n    with open(filepath) as fp:\n        data = json.load(fp)\n\n    data_series = data[\"data-series\"]\n\n    all_x, all_y = [], []\n    \n    if data['chart-type'] == 'horizontal_bar':\n        return None\n    \n#     if data['chart-type'] == 'scatter':\n#         return None\n    \n#     if data['chart-type'] == 'line':\n#         return None\n    for d in data_series:\n        x = d[\"x\"]\n        y = d[\"y\"]\n\n        x = round_float(x)\n        y = round_float(y)\n\n        # Ignore nan values\n        if is_nan(x) or is_nan(y):\n            continue\n\n        all_x.append(x)\n        all_y.append(y)\n        \n    \n    chart_type = f\"<{data['chart-type']}>\"\n    x_str = X_START + \";\".join(list(map(str, all_x))) + X_END\n    y_str = Y_START + \";\".join(list(map(str, all_y))) + Y_END\n    \n    gt_string = PROMPT_TOKEN + chart_type + x_str + y_str\n\n    return {\n        \"ground_truth\": gt_string,\n        \"x\": json.dumps(all_x),\n        \"y\": json.dumps(all_y),\n        \"chart-type\": data[\"chart-type\"],\n        \"id\": filepath.stem,\n        \"source\": data[\"source\"],\n    }","metadata":{"execution":{"iopub.status.busy":"2023-06-08T15:37:32.413932Z","iopub.execute_input":"2023-06-08T15:37:32.414319Z","iopub.status.idle":"2023-06-08T15:37:32.434074Z","shell.execute_reply.started":"2023-06-08T15:37:32.414285Z","shell.execute_reply":"2023-06-08T15:37:32.432977Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"get_gt_string_and_xy(data_dir / \"annotations\" / \"000d269c8e26.json\")","metadata":{"execution":{"iopub.status.busy":"2023-06-08T15:37:34.685116Z","iopub.execute_input":"2023-06-08T15:37:34.685655Z","iopub.status.idle":"2023-06-08T15:37:34.699305Z","shell.execute_reply.started":"2023-06-08T15:37:34.685610Z","shell.execute_reply":"2023-06-08T15:37:34.697972Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"{'ground_truth': '<|PROMPT|><line><x_start>0;2;4;6;8;10;12<x_end><y_start>45.8;45.9;46.3;46.1;46.1;47.0;47.4<y_end>',\n 'x': '[\"0\", \"2\", \"4\", \"6\", \"8\", \"10\", \"12\"]',\n 'y': '[\"45.8\", \"45.9\", \"46.3\", \"46.1\", \"46.1\", \"47.0\", \"47.4\"]',\n 'chart-type': 'line',\n 'id': '000d269c8e26',\n 'source': 'generated'}"},"metadata":{}}]},{"cell_type":"markdown","source":"# Creating the dataset","metadata":{}},{"cell_type":"code","source":"# This generator function will be used to create the Dataset object\n\n\ndef gen_data(files: List[Union[str, os.PathLike]]) -> Dict[str, str]:\n    \"\"\"\n    This function takes a list of json files and returns a generator that yields a\n    dictionary with the ground truth string and the path to the image.\n\n    Args:\n        files (list): A list of json files\n\n    Returns:\n        generator: A generator that yields a dictionary with the ground truth string and\n            the path to the corresponding image.\n    \"\"\"\n\n    for f in files:\n        res = get_gt_string_and_xy(f)\n        if res is None:\n            continue\n        yield {\n            **res,\n            \"image_path\": str(images_path / f\"{f.stem}.jpg\"),\n        }\n\n\nds = Dataset.from_generator(\n    gen_data, gen_kwargs={\"files\": train_json_files}, num_proc=CFG.num_proc\n)","metadata":{"execution":{"iopub.status.busy":"2023-06-08T15:37:38.842391Z","iopub.execute_input":"2023-06-08T15:37:38.843128Z","iopub.status.idle":"2023-06-08T15:39:11.260311Z","shell.execute_reply.started":"2023-06-08T15:37:38.843086Z","shell.execute_reply":"2023-06-08T15:39:11.259225Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Downloading and preparing dataset generator/default to /root/.cache/huggingface/datasets/generator/default-103496bb25861f1a/0.0.0...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Dataset generator downloaded and prepared to /root/.cache/huggingface/datasets/generator/default-103496bb25861f1a/0.0.0. Subsequent calls will reuse this data.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Checking image sizes","metadata":{}},{"cell_type":"code","source":"def add_image_sizes(examples: Dict[str, Union[str, os.PathLike]]) -> Dict[str, List[int]]:\n    \"\"\"\n    This function takes a dictionary of examples and adds the width and height of the\n    image to the dictionary. This is to be used with the `Dataset.map` function.\n\n    Args:\n        examples (dict): A dictionary of examples (from `map` function)\n\n    Returns:\n        dict: The dictionary with the width and height of the image added\n    \"\"\"\n\n    sizes = [Image.open(x).size for x in examples[\"image_path\"]]\n\n    width, height = list(zip(*sizes))\n\n    return {\n        \"width\": list(width),\n        \"height\": list(height),\n    }\n\n\nds = ds.map(add_image_sizes, batched=True, num_proc=CFG.num_proc)","metadata":{"execution":{"iopub.status.busy":"2023-06-08T15:39:15.052991Z","iopub.execute_input":"2023-06-08T15:39:15.053381Z","iopub.status.idle":"2023-06-08T15:42:08.121636Z","shell.execute_reply.started":"2023-06-08T15:39:15.053344Z","shell.execute_reply":"2023-06-08T15:42:08.120416Z"},"trusted":true},"execution_count":9,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map (num_proc=2):   0%|          | 0/60505 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}}]},{"cell_type":"code","source":"ds[0]","metadata":{"execution":{"iopub.status.busy":"2023-06-08T15:42:11.120559Z","iopub.execute_input":"2023-06-08T15:42:11.121044Z","iopub.status.idle":"2023-06-08T15:42:11.138194Z","shell.execute_reply.started":"2023-06-08T15:42:11.120998Z","shell.execute_reply":"2023-06-08T15:42:11.136878Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"{'ground_truth': '<|PROMPT|><line><x_start>10%;20%;30%;40%;50%;60%;70%<x_end><y_start>1.5481;11.5;14.0;12.2;7.9;1.7140;5.7<y_end>',\n 'x': '[\"10%\", \"20%\", \"30%\", \"40%\", \"50%\", \"60%\", \"70%\"]',\n 'y': '[\"1.5481\", \"11.5\", \"14.0\", \"12.2\", \"7.9\", \"1.7140\", \"5.7\"]',\n 'chart-type': 'line',\n 'id': 'cc68f19b708c',\n 'source': 'extracted',\n 'image_path': '/kaggle/input/benetech-making-graphs-accessible/train/images/cc68f19b708c.jpg',\n 'width': 515,\n 'height': 276}"},"metadata":{}}]},{"cell_type":"code","source":"# # Most fall within 560 x 560\n# plt.figure(figsize=(10, 10))\n\n# plt.scatter(x=ds[\"width\"], y=ds[\"height\"], marker=\"o\", alpha=0.5)\n\n# # Add axis labels\n# plt.xlabel(\"Width\")\n# plt.ylabel(\"Height\")\n\n# # Add a title\n# plt.title(\"Image Dimensions\")\n\n# # Add gridlines\n# plt.grid(True)\n\n# # Set aspect ratio to be equal\n# plt.gca().set_aspect(\"equal\", adjustable=\"box\")\n\n# # Show the plot\n# plt.show()","metadata":{"execution":{"iopub.status.busy":"2023-05-24T18:11:57.678025Z","iopub.execute_input":"2023-05-24T18:11:57.678475Z","iopub.status.idle":"2023-05-24T18:11:57.686133Z","shell.execute_reply.started":"2023-05-24T18:11:57.678432Z","shell.execute_reply":"2023-05-24T18:11:57.682537Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"# Model configuration\n\nNeed to set:  \n\n- image height\n- image width\n- max sequence length to generate","metadata":{}},{"cell_type":"code","source":"config = VisionEncoderDecoderConfig.from_pretrained(\"naver-clova-ix/donut-base\")\nconfig.encoder.image_size = (CFG.image_height, CFG.image_width)\nconfig.decoder.max_length = CFG.max_length\n\nprint(CFG.image_height, CFG.image_width, CFG.max_length)","metadata":{"execution":{"iopub.status.busy":"2023-06-08T15:42:18.971627Z","iopub.execute_input":"2023-06-08T15:42:18.972260Z","iopub.status.idle":"2023-06-08T15:42:19.303273Z","shell.execute_reply.started":"2023-06-08T15:42:18.972213Z","shell.execute_reply":"2023-06-08T15:42:19.302176Z"},"trusted":true},"execution_count":11,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (‚Ä¶)lve/main/config.json:   0%|          | 0.00/4.74k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9b00e509f8c14184be262eb1524522de"}},"metadata":{}},{"name":"stdout","text":"560 560 1024\n","output_type":"stream"}]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"# Checking tokenizer for unknowns\n","metadata":{}},{"cell_type":"code","source":"processor = DonutProcessor.from_pretrained(\"naver-clova-ix/donut-base\")\nprocessor.image_processor.size = {\n    \"height\": CFG.image_height,\n    \"width\": CFG.image_width,\n}","metadata":{"execution":{"iopub.status.busy":"2023-06-08T15:42:30.096183Z","iopub.execute_input":"2023-06-08T15:42:30.097270Z","iopub.status.idle":"2023-06-08T15:42:32.286564Z","shell.execute_reply.started":"2023-06-08T15:42:30.097228Z","shell.execute_reply":"2023-06-08T15:42:32.285308Z"},"trusted":true},"execution_count":12,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (‚Ä¶)rocessor_config.json:   0%|          | 0.00/362 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5dec3835afd049ebb968bf398b602374"}},"metadata":{}},{"name":"stderr","text":"Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading (‚Ä¶)okenizer_config.json:   0%|          | 0.00/518 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4d825cb0a2404e7d8943c33f87d535f2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (‚Ä¶)tencepiece.bpe.model:   0%|          | 0.00/1.30M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5e2ded141cbb4ed585c2a79b24625fe4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (‚Ä¶)/main/tokenizer.json:   0%|          | 0.00/4.01M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5d1441a5f16b4a7d9186c30e4c5710fe"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (‚Ä¶)in/added_tokens.json:   0%|          | 0.00/71.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e5a387834f5e47cab6a058859bcbc670"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (‚Ä¶)cial_tokens_map.json:   0%|          | 0.00/355 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"89bb20e8d80347ecbff902f41591c14e"}},"metadata":{}}]},{"cell_type":"code","source":"def check_for_unk(examples: Dict[str, str]) -> Dict[str, List[str]]:\n    \"\"\"\n    Check for unknown tokens in the given examples.\n\n    This function takes a dictionary containing a list of ground truth texts and\n    tokenizes them using the processor's tokenizer. It then checks for any unknown\n    tokens in the tokenized text and returns a dictionary containing a list of the\n    unknown tokens for each example.\n\n    Args:\n        examples (dict): A dictionary containing a list of ground truth texts. \n            Example: {\"ground_truth\": [\"text1\", \"text2\", ...]}\n\n    Returns:\n        dict: A dictionary containing a list of unknown tokens for each example. \n            Example: {\"unk_tokens\": [[\"unk1\", \"unk2\"], [], [\"unk3\"], ...]}\n    \"\"\"\n\n    texts = examples[\"ground_truth\"]\n\n    ids = processor.tokenizer(texts).input_ids\n    tokens = [processor.tokenizer.tokenize(x, add_special_tokens=True) for x in texts]\n\n    unk_tokens = []\n    for example_ids, example_tokens in zip(ids, tokens):\n        example_unk_tokens = []\n        for i in range(len(example_ids)):\n            if example_ids[i] == processor.tokenizer.unk_token_id:\n                example_unk_tokens.append(example_tokens[i])\n\n        unk_tokens.append(example_unk_tokens)\n\n    return {\"unk_tokens\": unk_tokens}\n\n\nunk = ds.map(check_for_unk, batched=True, num_proc=CFG.num_proc)","metadata":{"execution":{"iopub.status.busy":"2023-06-08T15:42:56.310360Z","iopub.execute_input":"2023-06-08T15:42:56.311416Z","iopub.status.idle":"2023-06-08T15:43:29.517714Z","shell.execute_reply.started":"2023-06-08T15:42:56.311372Z","shell.execute_reply":"2023-06-08T15:43:29.516502Z"},"trusted":true},"execution_count":13,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map (num_proc=2):   0%|          | 0/60505 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}}]},{"cell_type":"code","source":"# Let's look at only the examples with unknown tokens\nunk = unk.filter(lambda x: len(x[\"unk_tokens\"]) > 0, num_proc=CFG.num_proc)\n\nprint(len(unk))\n\nunk[0]","metadata":{"execution":{"iopub.status.busy":"2023-06-08T15:43:31.636378Z","iopub.execute_input":"2023-06-08T15:43:31.637532Z","iopub.status.idle":"2023-06-08T15:43:33.638007Z","shell.execute_reply.started":"2023-06-08T15:43:31.637463Z","shell.execute_reply":"2023-06-08T15:43:33.636810Z"},"trusted":true},"execution_count":14,"outputs":[{"output_type":"display_data","data":{"text/plain":"Filter (num_proc=2):   0%|          | 0/60505 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"37694\n","output_type":"stream"},{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"{'ground_truth': '<|PROMPT|><line><x_start>10%;20%;30%;40%;50%;60%;70%<x_end><y_start>1.5481;11.5;14.0;12.2;7.9;1.7140;5.7<y_end>',\n 'x': '[\"10%\", \"20%\", \"30%\", \"40%\", \"50%\", \"60%\", \"70%\"]',\n 'y': '[\"1.5481\", \"11.5\", \"14.0\", \"12.2\", \"7.9\", \"1.7140\", \"5.7\"]',\n 'chart-type': 'line',\n 'id': 'cc68f19b708c',\n 'source': 'extracted',\n 'image_path': '/kaggle/input/benetech-making-graphs-accessible/train/images/cc68f19b708c.jpg',\n 'width': 515,\n 'height': 276,\n 'unk_tokens': ['1']}"},"metadata":{}}]},{"cell_type":"code","source":"# Let's count which tokens show up most often as unknowns\nall_unk_tokens = [x for y in unk[\"unk_tokens\"] for x in y]\n\nCounter(all_unk_tokens)","metadata":{"execution":{"iopub.status.busy":"2023-05-31T09:12:16.686564Z","iopub.execute_input":"2023-05-31T09:12:16.687595Z","iopub.status.idle":"2023-05-31T09:12:16.862136Z","shell.execute_reply.started":"2023-05-31T09:12:16.687533Z","shell.execute_reply":"2023-05-31T09:12:16.860210Z"},"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"Counter({'1': 22860})"},"metadata":{}}]},{"cell_type":"markdown","source":"It seems that the donut tokenizer doesn't contain \"1\" which will result in many unknown tokens in the ground truth string. There is \"_1\" (space in front of 1) and \"01\", \"11\", \"21\", \"31\", \"41\" etc. But if there is a \"0.1\" or \">1\" then the 1 will turn into an unknown token.\n\nI'll handle this in the `preprocess` function.","metadata":{}},{"cell_type":"code","source":"example_str = \"0.1 1 1990\"\n\ntemp_ids = processor.tokenizer(example_str).input_ids\nprint(\"ids:\", temp_ids)\nprint(\"tokenized:\", processor.tokenizer.tokenize(example_str))\nprint(\"decoded:\", processor.tokenizer.decode(temp_ids))\nprint(\"unk id:\", processor.tokenizer.unk_token_id)\n\n# Adding these tokens should mean that there should be very few unknown tokens\nnum_added = processor.tokenizer.add_tokens([\"<one>\"] + new_tokens)\nprint(num_added, \"tokens added\")\n\nconfig.pad_token_id = processor.tokenizer.pad_token_id\nconfig.decoder_start_token_id = processor.tokenizer.convert_tokens_to_ids([PROMPT_TOKEN])[0]","metadata":{"execution":{"iopub.status.busy":"2023-06-08T15:43:38.349777Z","iopub.execute_input":"2023-06-08T15:43:38.350262Z","iopub.status.idle":"2023-06-08T15:43:38.363390Z","shell.execute_reply.started":"2023-06-08T15:43:38.350208Z","shell.execute_reply":"2023-06-08T15:43:38.362151Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"ids: [0, 50891, 39539, 3, 1314, 4314, 2]\ntokenized: ['‚ñÅ0', '.', '1', '‚ñÅ1', '‚ñÅ1990']\ndecoded: <s> 0.<unk> 1 1990</s>\nunk id: 3\n10 tokens added\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Preprocessing function to get pixel values and input_ids\n\nWhen ever the `_getitem__` function is called for the dataset, it will run this function.","metadata":{}},{"cell_type":"code","source":"from functools import partial\n\none_token_id = processor.tokenizer(\"<one>\", add_special_tokens=False).input_ids[0]\nunk_token_id = processor.tokenizer.unk_token_id\n\n\ndef replace_unk_tokens_with_one(example_ids: List[int], example_tokens: List[str], one_token_id:int, unk_token_id:int) -> List[int]:\n    \"\"\"\n    Replace unknown tokens that represent \"1\" with the correct token id.\n\n    Args:\n        example_ids (list): List of token ids for a given example\n        example_tokens (list): List of tokens for the same given example\n        one_token_id (int): Token id for the \"<one>\" token\n        unk_token_id (int): Token id for the unknown token\n\n    Returns:\n        list: The updated list of token ids with the correct token id for \"1\"\n    \"\"\"\n    \n    temp_ids = []\n    for id_, token in zip(example_ids, example_tokens):\n        if id_ == unk_token_id and token == \"1\":\n            id_ = one_token_id\n        temp_ids.append(id_)\n    return temp_ids\n\n\ndef preprocess(examples: Dict[str, str], processor: DonutProcessor, CFG: CFG) -> Dict[str, Union[torch.Tensor, List[int], List[str]]]:\n    \"\"\"\n    Preprocess the given examples.\n\n    This function processes the input examples by tokenizing the texts, replacing\n    any unknown tokens that represent \"1\" with the correct token id, and loading\n    the images.\n\n    Args:\n        examples (dict): A dictionary containing ground truth texts, image paths, and ids\n        processor: An object responsible for tokenizing texts and processing images\n        CFG: A configuration object containing settings and hyperparameters\n\n    Returns:\n        dict: A dictionary containing preprocessed images, token ids, and ids\n    \"\"\"\n    \n    pixel_values = []\n\n    texts = examples[\"ground_truth\"]\n\n    ids = processor.tokenizer(\n        texts,\n        add_special_tokens=False,\n        max_length=CFG.max_length,\n        padding=True,\n        truncation=True,\n    ).input_ids\n\n    if isinstance(texts, str):\n        texts = [texts]\n\n    tokens = [processor.tokenizer.tokenize(text, add_special_tokens=False) for text in texts]\n    \n    one_token_id = processor.tokenizer(\"<one>\", add_special_tokens=False).input_ids[0]\n    unk_token_id = processor.tokenizer.unk_token_id\n    \n    final_ids = [\n        replace_unk_tokens_with_one(example_ids, example_tokens, one_token_id, unk_token_id)\n        for example_ids, example_tokens in zip(ids, tokens)\n    ]\n\n    for sample in examples[\"image_path\"]:\n        pixel_values.append(processor(sample, random_padding=True).pixel_values)\n\n    return {\n        \"pixel_values\": torch.tensor(np.vstack(pixel_values)),\n        \"input_ids\": final_ids,\n        \"id\": examples[\"id\"],\n    }\n\n\nimage_ds = ds.cast_column(\"image_path\", ds_img())\nimage_ds.set_transform(partial(preprocess, processor=processor, CFG=CFG))","metadata":{"execution":{"iopub.status.busy":"2023-06-08T15:43:40.628483Z","iopub.execute_input":"2023-06-08T15:43:40.629682Z","iopub.status.idle":"2023-06-08T15:43:40.699495Z","shell.execute_reply.started":"2023-06-08T15:43:40.629630Z","shell.execute_reply":"2023-06-08T15:43:40.698465Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"sample = image_ds[[0, 1, 2]]\n\nprint(sample[\"pixel_values\"].shape)\nprint(processor.decode(sample[\"input_ids\"][2]))\nprint(len(sample[\"input_ids\"][2]))\nprint(processor.tokenizer.convert_ids_to_tokens(sample[\"input_ids\"][2]))","metadata":{"execution":{"iopub.status.busy":"2023-06-08T15:43:44.437240Z","iopub.execute_input":"2023-06-08T15:43:44.437713Z","iopub.status.idle":"2023-06-08T15:43:44.671201Z","shell.execute_reply.started":"2023-06-08T15:43:44.437658Z","shell.execute_reply":"2023-06-08T15:43:44.669847Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"torch.Size([3, 3, 560, 560])\n<|PROMPT|><scatter><x_start> 1990.<one>;1991.0;1991.9;1992.9;1993.9;1994.9;1996.0;1996.9;1997.9;1999.0;1999.9;2000.9;2001.9;2002.8;2003.9;2004.9;2005.8;2006.9;2008.<one>;2008.9;2009.9;2011.0;2011.8;2012.8;2013.9;2014.9;2015.9;2016.9;2017.9;2019.0<x_end><y_start> 244.2;201.9;186.<one>;214.<one>;194.7;176.8;174.6;206.2;158.<one>;163.1;385.5;244.9;251.4;363.3;301.6;201.9;352.5;196.<one>;221.2;272.9;156.0;153.8;65.6;47.6;63.4;70.6;45.5;71.3;33.3;28.2<y_end>\n244\n['<|PROMPT|>', '<scatter>', '<x_start>', '‚ñÅ1990.', '<one>', ';', '1991', '.', '0', ';', '1991', '.', '9', ';', '1992', '.', '9', ';', '1993', '.', '9', ';', '1994', '.', '9', ';', '1996', '.', '0', ';', '1996', '.', '9', ';', '1997', '.', '9', ';', '1999', '.', '0', ';', '1999', '.', '9', ';', '2000', '.', '9', ';', '2001', '.', '9', ';', '200', '2.8', ';', '2003', '.', '9', ';', '2004', '.', '9', ';', '200', '5.8', ';', '2006', '.', '9', ';', '2008', '.', '<one>', ';', '2008', '.', '9', ';', '2009', '.', '9', ';', '2011', '.', '0', ';', '2011', '.', '8', ';', '2012', '.', '8', ';', '2013', '.', '9', ';', '2014', '.', '9', ';', '2015', '.', '9', ';', '2016', '.', '9', ';', '2017', '.', '9', ';', '2019', '.', '0', '<x_end>', '<y_start>', '‚ñÅ24', '4.2', ';', '2', '01', '.', '9', ';', '18', '6', '.', '<one>', ';', '2', '14', '.', '<one>', ';', '19', '4', '.', '7', ';', '17', '6', '.', '8', ';', '17', '4', '.', '6', ';', '20', '6.2', ';', '15', '8', '.', '<one>', ';', '16', '3.1', ';', '3', '85', '.', '5', ';', '24', '4', '.', '9', ';', '25', '1.4', ';', '36', '3.3', ';', '3', '01', '.', '6', ';', '2', '01', '.', '9', ';', '35', '2.5', ';', '19', '6', '.', '<one>', ';', '22', '1.2', ';', '27', '2.', '9', ';', '15', '6', '.', '0', ';', '15', '3', '.', '8', ';', '6', '5.6', ';', '4', '7.6', ';', '63', '.', '4', ';', '70', '.', '6', ';', '45', '.', '5', ';', '7', '1.3', ';', '3', '3.3', ';', '28', '.', '2', '<y_end>']\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Creating splits\n\n\nBasic split: break extracted into folds, all generated and some extracted into train. Only extracted in validation.","metadata":{}},{"cell_type":"code","source":"# separate by extracted and generated\n\nextracted_ds = ds.filter(lambda x: x[\"source\"] == \"extracted\", num_proc=CFG.num_proc)\ngenerated_ds = ds.filter(lambda x: x[\"source\"] == \"generated\", num_proc=CFG.num_proc)\n\nchart_types = extracted_ds[\"chart-type\"]\n\nlst_save = []\nlst_save1 = []\nfor idx,enum in enumerate(generated_ds):\n    if enum[\"chart-type\"] == 'dot':\n        if len(lst_save) != 100:\n            lst_save.append(idx)\n            continue\n        if len(lst_save) == 100:\n            lst_save1.append(idx)\n            if len(lst_save1) == 50:\n                break\n            \ntrain_save = []\nfor idx,enum in enumerate(generated_ds):\n    if enum[\"chart-type\"] == 'dot':\n        if idx in lst_save or idx in lst_save1:\n            continue\n    train_save.append(idx)\n    \nprint(lst_save)\n\nprint(Counter(chart_types))\n\nskf = StratifiedKFold(n_splits=4)\n\nfold_idxs = []\n\nfor _, val_idxs in skf.split(chart_types, y=chart_types):\n    fold_idxs.append(val_idxs)\n    \nfor n, idxs in enumerate(fold_idxs):\n    print(Counter([chart_types[i] for i in idxs]))\n    if n > 3:\n        break","metadata":{"execution":{"iopub.status.busy":"2023-06-08T15:43:50.282217Z","iopub.execute_input":"2023-06-08T15:43:50.282643Z","iopub.status.idle":"2023-06-08T15:44:03.694074Z","shell.execute_reply.started":"2023-06-08T15:43:50.282603Z","shell.execute_reply":"2023-06-08T15:44:03.692755Z"},"trusted":true},"execution_count":18,"outputs":[{"output_type":"display_data","data":{"text/plain":"Filter (num_proc=2):   0%|          | 0/60505 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Filter (num_proc=2):   0%|          | 0/60505 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"[5, 12, 16, 23, 30, 37, 53, 61, 66, 91, 102, 115, 140, 141, 152, 157, 169, 183, 196, 218, 222, 262, 264, 276, 309, 315, 327, 343, 348, 360, 369, 378, 391, 407, 411, 445, 449, 456, 457, 458, 485, 497, 498, 500, 512, 536, 602, 632, 643, 646, 651, 658, 666, 677, 689, 695, 699, 727, 755, 771, 776, 784, 787, 815, 818, 820, 821, 850, 856, 871, 876, 883, 887, 903, 913, 931, 977, 1003, 1008, 1026, 1040, 1076, 1079, 1080, 1108, 1122, 1126, 1135, 1156, 1161, 1172, 1210, 1217, 1223, 1247, 1248, 1265, 1269, 1292, 1298]\nCounter({'vertical_bar': 457, 'line': 423, 'scatter': 165})\nCounter({'vertical_bar': 115, 'line': 106, 'scatter': 41})\nCounter({'vertical_bar': 114, 'line': 106, 'scatter': 41})\nCounter({'vertical_bar': 114, 'line': 106, 'scatter': 41})\nCounter({'vertical_bar': 114, 'line': 105, 'scatter': 42})\n","output_type":"stream"}]},{"cell_type":"code","source":"len(train_save)","metadata":{"execution":{"iopub.status.busy":"2023-06-08T15:44:03.696525Z","iopub.execute_input":"2023-06-08T15:44:03.697253Z","iopub.status.idle":"2023-06-08T15:44:03.704998Z","shell.execute_reply.started":"2023-06-08T15:44:03.697200Z","shell.execute_reply":"2023-06-08T15:44:03.703868Z"},"trusted":true},"execution_count":19,"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"59310"},"metadata":{}}]},{"cell_type":"code","source":"# print(type(val_set))\n# print(type(ds))","metadata":{"execution":{"iopub.status.busy":"2023-05-24T18:12:08.330825Z","iopub.execute_input":"2023-05-24T18:12:08.331490Z","iopub.status.idle":"2023-05-24T18:12:08.340018Z","shell.execute_reply.started":"2023-05-24T18:12:08.331450Z","shell.execute_reply":"2023-05-24T18:12:08.338949Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"# Combine generated and some of the extracted examples into the train set\n\nfrom datasets import concatenate_datasets\n\nfold = 0\nfold1 = 1\n\ntrain_extracted = extracted_ds.select(\n    list(chain(*[x for i, x in enumerate(fold_idxs) if i != fold and i != fold1]))\n)\n\ntrain_ds = concatenate_datasets([train_extracted, generated_ds.select(train_save)])\ntrain_ds = train_ds.cast_column(\"image_path\", ds_img())\ntrain_ds.set_transform(partial(preprocess, processor=processor, CFG=CFG))\n\n# Creat validation set from only extracted examples\n\nval_gt_ds = concatenate_datasets([extracted_ds.select(fold_idxs[fold]),generated_ds.select(lst_save)])\nval_ds = val_gt_ds.cast_column(\"image_path\", ds_img())\nval_ds.set_transform(partial(preprocess, processor=processor, CFG=CFG))\n\ntest_gt_ds = concatenate_datasets([extracted_ds.select(fold_idxs[fold1]),generated_ds.select(lst_save1)])\ntest_ds = test_gt_ds.cast_column(\"image_path\", ds_img())\ntest_ds.set_transform(partial(preprocess, processor=processor, CFG=CFG))\n\ngt_chart_type = val_gt_ds[\"chart-type\"]\ngt_x = [json.loads(_) for _ in val_gt_ds[\"x\"]]\ngt_y = [json.loads(_) for _ in val_gt_ds[\"y\"]]\ngt_ids = val_gt_ds[\"id\"]\n\ngt_test_chart_type = test_gt_ds[\"chart-type\"]\ngt_test_x = [json.loads(_) for _ in test_gt_ds[\"x\"]]\ngt_test_y = [json.loads(_) for _ in test_gt_ds[\"y\"]]\ngt_test_ids = test_gt_ds[\"id\"]\n\ni = 0\nprint(gt_chart_type[i])\nprint(gt_x[i])\nprint(gt_y[i])\nprint(gt_ids[i])\nprint(Counter(gt_chart_type))\nprint(Counter(gt_test_chart_type))","metadata":{"execution":{"iopub.status.busy":"2023-06-08T15:44:09.637236Z","iopub.execute_input":"2023-06-08T15:44:09.637756Z","iopub.status.idle":"2023-06-08T15:44:10.257525Z","shell.execute_reply.started":"2023-06-08T15:44:09.637714Z","shell.execute_reply":"2023-06-08T15:44:10.256422Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"line\n['10%', '20%', '30%', '40%', '50%', '60%', '70%']\n['1.5481', '11.5', '14.0', '12.2', '7.9', '1.7140', '5.7']\ncc68f19b708c\nCounter({'vertical_bar': 115, 'line': 106, 'dot': 100, 'scatter': 41})\nCounter({'vertical_bar': 114, 'line': 106, 'dot': 50, 'scatter': 41})\n","output_type":"stream"}]},{"cell_type":"code","source":"import pickle\n\nwith open('filename.pickle', 'wb') as handle:\n    pickle.dump(gt_test_ids, handle, protocol=pickle.HIGHEST_PROTOCOL)","metadata":{"execution":{"iopub.status.busy":"2023-06-08T15:48:13.819142Z","iopub.execute_input":"2023-06-08T15:48:13.819898Z","iopub.status.idle":"2023-06-08T15:48:13.826376Z","shell.execute_reply.started":"2023-06-08T15:48:13.819855Z","shell.execute_reply":"2023-06-08T15:48:13.825289Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"markdown","source":"# Collate function to make sure the ids are all the same length in a batch","metadata":{}},{"cell_type":"code","source":"pad_token_id = processor.tokenizer.pad_token_id\n\n\ndef collate_fn(samples: List[Dict[str, Union[torch.Tensor, List[int], str]]]) -> Dict[str, Union[torch.Tensor, List[str]]]:\n    \"\"\"\n    Custom collate function for DataLoader.\n\n    This function takes a list of samples and combines them into a batch with\n    properly padded input_ids.\n\n    Args:\n        samples (List[Dict[str, Union[torch.Tensor, List[int], str]]]): \n            A list of samples, where each sample is a dictionary containing\n            \"pixel_values\" (torch.Tensor), \"input_ids\" (List[int]), and \"id\" (str).\n\n    Returns:\n        Dict[str, Union[torch.Tensor, List[str]]]: \n            A dictionary containing the combined pixel values, padded input_ids, and ids.\n    \"\"\"\n\n    batch = {}\n\n    batch[\"pixel_values\"] = torch.stack([x[\"pixel_values\"] for x in samples])\n\n    max_length = max([len(x[\"input_ids\"]) for x in samples])\n\n    # Make a multiple of 8 to efficiently use the tensor cores\n    if max_length % 8 != 0:\n        max_length = (max_length // 8 + 1) * 8\n\n    input_ids = [\n        x[\"input_ids\"] + [pad_token_id] * (max_length - len(x[\"input_ids\"]))\n        for x in samples\n    ]\n\n    labels = torch.tensor(input_ids)\n    labels[labels == pad_token_id] = -100 # ignore loss on padding tokens\n    batch[\"labels\"] = labels\n    \n    batch[\"id\"] = [x[\"id\"] for x in samples]\n\n    return batch\n","metadata":{"execution":{"iopub.status.busy":"2023-05-31T09:12:42.345865Z","iopub.execute_input":"2023-05-31T09:12:42.346980Z","iopub.status.idle":"2023-05-31T09:12:42.359161Z","shell.execute_reply.started":"2023-05-31T09:12:42.346925Z","shell.execute_reply":"2023-05-31T09:12:42.357837Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"# Dataloaders\n\nValidation uses generation so it is very slow. That's why I only use a small fraction of the examples.","metadata":{}},{"cell_type":"code","source":"if CFG.debug:\n    train_ds = train_ds.select(range(100))\n    val_ds = val_ds.select(range(100))\n\ntrain_dataloader = DataLoader(\n    train_ds,\n    batch_size=CFG.batch_size,\n    shuffle=True,\n    collate_fn=collate_fn,\n    num_workers=CFG.num_workers,\n)\nval_dataloader = DataLoader(\n    val_ds,\n    batch_size=CFG.batch_size,\n    shuffle=False,\n    collate_fn=collate_fn,\n    num_workers=CFG.num_workers,\n)\n\ntest_dataloader = DataLoader(\n    test_ds,\n    batch_size=CFG.batch_size,\n    shuffle=False,\n    collate_fn=collate_fn,\n    num_workers=CFG.num_workers,\n)\n\nnum_training_steps = len(train_dataloader) * CFG.epochs // CFG.gpus\n\nbatch = next(iter(train_dataloader))\n\nbatch.keys(), [(k, v.shape) for k, v in batch.items() if k != \"id\"]","metadata":{"execution":{"iopub.status.busy":"2023-05-31T09:12:44.046585Z","iopub.execute_input":"2023-05-31T09:12:44.047346Z","iopub.status.idle":"2023-05-31T09:12:44.546079Z","shell.execute_reply.started":"2023-05-31T09:12:44.047303Z","shell.execute_reply":"2023-05-31T09:12:44.544872Z"},"trusted":true},"execution_count":20,"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"(dict_keys(['pixel_values', 'labels', 'id']),\n [('pixel_values', torch.Size([2, 3, 560, 560])),\n  ('labels', torch.Size([2, 128]))])"},"metadata":{}}]},{"cell_type":"markdown","source":"# Functions to calculate metrics","metadata":{}},{"cell_type":"code","source":"def rmse(y_true: List[float], y_pred: List[float]) -> float:\n    \"\"\"\n    Calculate the Root Mean Square Error (RMSE) between the true and predicted values.\n\n    Args:\n        y_true (List[float]): The true values.\n        y_pred (List[float]): The predicted values.\n\n    Returns:\n        float: The Root Mean Square Error.\n    \"\"\"\n    return np.sqrt(np.mean(np.square(np.subtract(y_true, y_pred))))\n\n\ndef sigmoid(x: float) -> float:\n    \"\"\"\n    Calculate the sigmoid function for the given value.\n\n    Args:\n        x (float): The input value.\n\n    Returns:\n        float: The result of the sigmoid function.\n    \"\"\"\n    return 2 - 2 / (1 + np.exp(-x))\n\n\ndef normalized_rmse(y_true: List[float], y_pred: List[float]) -> float:\n    \"\"\"\n    Calculate the normalized Root Mean Square Error (RMSE) between the true and predicted values.\n\n    Args:\n        y_true (List[float]): The true values.\n        y_pred (List[float]): The predicted values.\n\n    Returns:\n        float: The normalized Root Mean Square Error.\n    \"\"\"\n    numerator = rmse(y_true, y_pred)\n    denominator = rmse(y_true, np.mean(y_true))\n\n    # https://www.kaggle.com/competitions/benetech-making-graphs-accessible/discussion/396947\n    if denominator == 0:\n        if numerator == 0:\n            return 1.0\n        return 0.0\n\n    return sigmoid(numerator / denominator)\n\n\ndef normalized_levenshtein_score(y_true: List[str], y_pred: List[str]) -> float:\n    \"\"\"\n    Calculate the normalized Levenshtein distance between two lists of strings.\n\n    Args:\n        y_true (List[str]): The true values.\n        y_pred (List[str]): The predicted values.\n\n    Returns:\n        float: The normalized Levenshtein distance.\n    \"\"\"\n    total_distance = np.sum([levenshtein(yt, yp) for yt, yp in zip(y_true, y_pred)])\n    length_sum = np.sum([len(yt) for yt in y_true])\n    return sigmoid(total_distance / length_sum)\n\n\ndef score_series(\n    y_true: List[Union[float, str]], y_pred: List[Union[float, str]]\n) -> float:\n    \"\"\"\n    Calculate the score for a series of true and predicted values.\n\n    Args:\n        y_true (List[Union[float, str]]): The true values.\n        y_pred (List[Union[float, str]]): The predicted values.\n\n    Returns:\n        float: The score for the series.\n    \"\"\"\n    if len(y_true) != len(y_pred):\n        return 0.0\n    if isinstance(y_true[0], str):\n        return normalized_levenshtein_score(y_true, y_pred)\n    else:\n        # Since this is a generative model, there is a chance it doesn't produce a float.\n        # In that case, we return 0.0.\n        try:\n            return normalized_rmse(y_true, list(map(float, y_pred)))\n        except:\n            return 0.0\n\n\ndef benetech_score(ground_truth: pd.DataFrame, predictions: pd.DataFrame) -> float:\n    \"\"\"Evaluate predictions using the metric from the Benetech - Making Graphs Accessible.\n\n    Parameters\n    ----------\n    ground_truth: pd.DataFrame\n        Has columns `[data_series, chart_type]` and an index `id`. Values in `data_series`\n        should be either arrays of floats or arrays of strings.\n\n    predictions: pd.DataFrame\n    \"\"\"\n    if not ground_truth.index.equals(predictions.index):\n        raise ValueError(\n            \"Must have exactly one prediction for each ground-truth instance.\"\n        )\n    if not ground_truth.columns.equals(predictions.columns):\n        raise ValueError(f\"Predictions must have columns: {ground_truth.columns}.\")\n    pairs = zip(\n        ground_truth.itertuples(index=False), predictions.itertuples(index=False)\n    )\n    scores = []\n    for (gt_series, gt_type), (pred_series, pred_type) in pairs:\n        if gt_type != pred_type:  # Check chart_type condition\n            scores.append(0.0)\n        else:  # Score with RMSE or Levenshtein as appropriate\n            scores.append(score_series(gt_series, pred_series))\n\n    ground_truth[\"score\"] = scores\n\n    grouped = ground_truth.groupby(\"chart_type\", as_index=False)[\"score\"].mean()\n\n    chart_type2score = {\n        chart_type: score\n        for chart_type, score in zip(grouped[\"chart_type\"], grouped[\"score\"])\n    }\n\n    return np.mean(scores), chart_type2score\n\n\ndef string2triplet(pred_string: str) -> Tuple[str, List[str], List[str]]:\n    \"\"\"\n    Convert a prediction string to a triplet of chart type, x values, and y values.\n\n    Args:\n        pred_string (str): The prediction string.\n\n    Returns:\n        Tuple[str, List[str], List[str]]: A triplet of chart type, x values, and y values.\n    \"\"\"\n    \n    chart_type = \"line\"\n    for tok in CHART_TYPE_TOKENS:\n        if tok in pred_string:\n            chart_type = tok.strip(\"<>\")\n\n    pred_string = re.sub(r\"<one>\", \"1\", pred_string)\n\n    x = pred_string.split(X_START)[1].split(X_END)[0].split(\";\")\n    y = pred_string.split(Y_START)[1].split(Y_END)[0].split(\";\")\n\n    if len(x) == 0 or len(y) == 0:\n        return chart_type, [], []\n\n    min_length = min(len(x), len(y))\n\n    x = x[:min_length]\n    y = y[:min_length]\n\n    return chart_type, x, y\n\n\ndef validation_metrics(val_outputs: List[str], val_ids: List[str], gt_df: pd.DataFrame) -> Dict[str, float]:\n    \"\"\"\n    Calculate validation metrics for a set of outputs, ids, and ground truth dataframe.\n\n    Args:\n        val_outputs (List[str]): A list of validation outputs.\n        val_ids (List[str]): A list of validation ids.\n        gt_df (pd.DataFrame): The ground truth dataframe.\n\n    Returns:\n        Dict[str, float]: A dictionary containing the validation scores.\n    \"\"\"\n    pred_triplets = []\n\n    for example_output in val_outputs:\n\n        if not all([x in example_output for x in [X_START, X_END, Y_START, Y_END]]):\n            pred_triplets.append((\"line\", [], []))\n        else:\n            pred_triplets.append(string2triplet(example_output))\n\n    pred_df = pd.DataFrame(\n        index=[f\"{id_}_x\" for id_ in val_ids] + [f\"{id_}_y\" for id_ in val_ids],\n        data={\n            \"data_series\": [x[1] for x in pred_triplets]\n            + [x[2] for x in pred_triplets],\n            \"chart_type\": [x[0] for x in pred_triplets] * 2,\n        },\n    )\n\n    overall_score, chart_type2score = benetech_score(\n        gt_df.loc[pred_df.index.values], pred_df\n    )\n\n    return {\n        \"val_score\": overall_score,\n        **{f\"{k}_score\": v for k, v in chart_type2score.items()},\n    }\n","metadata":{"execution":{"iopub.status.busy":"2023-05-31T09:12:48.810724Z","iopub.execute_input":"2023-05-31T09:12:48.811140Z","iopub.status.idle":"2023-05-31T09:12:48.848723Z","shell.execute_reply.started":"2023-05-31T09:12:48.811100Z","shell.execute_reply":"2023-05-31T09:12:48.847197Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":"# Lightning Module\n\nIt will use generation for validation which can be very slow. If you want to utilize the two GPUs on Kaggle, you should probably not run it in a notebook because DDP gets funky when doing that.","metadata":{}},{"cell_type":"code","source":"import pytorch_lightning as pl\nfrom transformers import PreTrainedTokenizerBase, PreTrainedModel\n\nclass DonutModelPLModule(pl.LightningModule):\n    def __init__(self, processor: PreTrainedTokenizerBase, model: PreTrainedModel, gt_df: pd.DataFrame, num_training_steps: int):\n        \"\"\"\n        A PyTorch Lightning module for the DonutModel.\n\n        Args:\n            processor (PreTrainedTokenizerBase): The tokenizer/processor for the model.\n            model (PreTrainedModel): The pretrained model.\n            gt_df (pd.DataFrame): The ground truth dataframe.\n            num_training_steps (int): The number of training steps.\n        \"\"\"\n        super().__init__()\n        self.processor = processor\n        self.model = model\n        self.gt_df = gt_df\n        self.num_training_steps = num_training_steps\n\n    def training_step(self, batch: Dict[str, torch.Tensor], batch_idx: int) -> torch.Tensor:\n        pixel_values = batch[\"pixel_values\"]\n        labels = batch[\"labels\"]\n\n        outputs = self.model(pixel_values, labels=labels)\n        loss = outputs.loss\n        self.log(\"train_loss\", loss)\n        return loss\n\n    def validation_step(self, batch: Dict[str, torch.Tensor], batch_idx: int, dataset_idx: int = 0) -> None:\n        pixel_values = batch[\"pixel_values\"]\n        batch_size = pixel_values.shape[0]\n        # we feed the prompt to the model\n        decoder_input_ids = torch.full(\n            (batch_size, 1),\n            self.model.config.decoder_start_token_id,\n            device=self.device,\n        )\n\n        outputs = self.model.generate(\n            pixel_values,\n            decoder_input_ids=decoder_input_ids,\n            max_length=CFG.max_length,\n            early_stopping=True,\n            pad_token_id=self.processor.tokenizer.pad_token_id,\n            eos_token_id=self.processor.tokenizer.eos_token_id,\n            use_cache=True,\n            num_beams=1,\n            top_k=1,\n            bad_words_ids=[[self.processor.tokenizer.unk_token_id]],\n            return_dict_in_generate=True,\n        )\n\n        self.val_outputs.extend(\n            self.processor.tokenizer.batch_decode(outputs.sequences)\n        )\n        self.val_ids.extend(batch[\"id\"])\n\n    def on_validation_start(self) -> None:\n        self.val_outputs, self.val_ids = [], []\n\n    def validation_epoch_end(self, outputs: List[Any]) -> None:\n\n        metrics = validation_metrics(self.val_outputs, self.val_ids, self.gt_df)\n        print(\"\\n\", metrics)\n\n        self.log_dict(metrics)\n\n        self.val_outputs, self.val_ids = [], []\n\n    def configure_optimizers(self) -> torch.optim.Optimizer:\n        \n        optimizer = torch.optim.Adam(self.parameters(), lr=CFG.lr)\n\n        return optimizer","metadata":{"execution":{"iopub.status.busy":"2023-05-31T09:12:54.326968Z","iopub.execute_input":"2023-05-31T09:12:54.327411Z","iopub.status.idle":"2023-05-31T09:12:55.675827Z","shell.execute_reply.started":"2023-05-31T09:12:54.327339Z","shell.execute_reply":"2023-05-31T09:12:55.674754Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":"# Create model and ground truth dataframe","metadata":{}},{"cell_type":"code","source":"gt_chart_type = val_gt_ds[\"chart-type\"]\ngt_x = [json.loads(_) for _ in val_gt_ds[\"x\"]]\ngt_y = [json.loads(_) for _ in val_gt_ds[\"y\"]]\ngt_ids = val_gt_ds[\"id\"]\n\nindex = [f\"{id_}_x\" for id_ in gt_ids] + [f\"{id_}_y\" for id_ in gt_ids]\ngt_df = pd.DataFrame(\n    index=index,\n    data={\n        \"data_series\": gt_x + gt_y,\n        \"chart_type\": gt_chart_type * 2,\n    },\n)\n\n\nmodel = VisionEncoderDecoderModel.from_pretrained(\n    \"naver-clova-ix/donut-base\", config=config\n)\nmodel.decoder.resize_token_embeddings(len(processor.tokenizer))\nmodel_module = DonutModelPLModule(processor, model, gt_df, num_training_steps)","metadata":{"execution":{"iopub.status.busy":"2023-05-31T09:13:00.456668Z","iopub.execute_input":"2023-05-31T09:13:00.457817Z","iopub.status.idle":"2023-05-31T09:13:12.557272Z","shell.execute_reply.started":"2023-05-31T09:13:00.457755Z","shell.execute_reply":"2023-05-31T09:13:12.556115Z"},"trusted":true},"execution_count":23,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading pytorch_model.bin:   0%|          | 0.00/809M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"036ebe7520f6482597755af26d82362b"}},"metadata":{}}]},{"cell_type":"code","source":"model = VisionEncoderDecoderModel.from_pretrained(\n    \"hoangphu7122002ai/donutExtractBarDot\"\n)\nprocessor = DonutProcessor.from_pretrained(\"hoangphu7122002ai/donutExtractBarDot\")\nmodel_module = DonutModelPLModule(processor, model, gt_df, num_training_steps)","metadata":{"execution":{"iopub.status.busy":"2023-05-31T17:23:21.077522Z","iopub.execute_input":"2023-05-31T17:23:21.078732Z","iopub.status.idle":"2023-05-31T17:23:43.419043Z","shell.execute_reply.started":"2023-05-31T17:23:21.078678Z","shell.execute_reply":"2023-05-31T17:23:43.417674Z"},"trusted":true},"execution_count":29,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (‚Ä¶)lve/main/config.json:   0%|          | 0.00/5.03k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d6d1553354ef407b99c9562bcb89e888"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading pytorch_model.bin:   0%|          | 0.00/809M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c9e7ca8f98c644f1988316a16e7d08c0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (‚Ä¶)neration_config.json:   0%|          | 0.00/217 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"835152ad9d0c4449ab8e4f92ad83ce08"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (‚Ä¶)rocessor_config.json:   0%|          | 0.00/438 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c73a6b962ad94526a543de612b6d595f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (‚Ä¶)okenizer_config.json:   0%|          | 0.00/510 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"877a765a0ebe4b02b85be3ea573861f5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (‚Ä¶)tencepiece.bpe.model:   0%|          | 0.00/1.30M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c7cbe37ff7a34a4eb24f000603fe0575"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (‚Ä¶)/main/tokenizer.json:   0%|          | 0.00/4.01M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3f72539bec0e46a6abd4d00201263842"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (‚Ä¶)in/added_tokens.json:   0%|          | 0.00/260 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a7eb29a3ec194a31a6df8ed02df32830"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (‚Ä¶)cial_tokens_map.json:   0%|          | 0.00/355 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"863b134f60d84bfe8f473873be799051"}},"metadata":{}}]},{"cell_type":"code","source":"Counter(val_gt_ds['chart-type'])","metadata":{"execution":{"iopub.status.busy":"2023-05-31T09:13:33.834062Z","iopub.execute_input":"2023-05-31T09:13:33.834510Z","iopub.status.idle":"2023-05-31T09:13:33.847361Z","shell.execute_reply.started":"2023-05-31T09:13:33.834469Z","shell.execute_reply":"2023-05-31T09:13:33.845888Z"},"trusted":true},"execution_count":25,"outputs":[{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"Counter({'vertical_bar': 115, 'dot': 100})"},"metadata":{}}]},{"cell_type":"markdown","source":"# Train!","metadata":{}},{"cell_type":"code","source":"!python -c \"from huggingface_hub.hf_api import HfFolder; HfFolder.save_token('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')\"","metadata":{"execution":{"iopub.status.busy":"2023-05-31T09:13:40.364676Z","iopub.execute_input":"2023-05-31T09:13:40.365085Z","iopub.status.idle":"2023-05-31T09:13:41.705702Z","shell.execute_reply.started":"2023-05-31T09:13:40.365045Z","shell.execute_reply":"2023-05-31T09:13:41.703928Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"from pytorch_lightning.callbacks import ModelCheckpoint\nfrom pytorch_lightning.loggers import WandbLogger\nfrom pytorch_lightning.callbacks import Callback, EarlyStopping\n\n# checkpoint_cb = ModelCheckpoint(CFG.output_path)\n\nloggers = []\nif CFG.use_wandb:\n    import wandb\n    wandb.finish()\n    from kaggle_secrets import UserSecretsClient\n    user_secrets = UserSecretsClient()\n    key = user_secrets.get_secret(\"donut\")\n    wandb.login(key=key)\n    \n    loggers.append(WandbLogger(project=\"benetech\"))\n\nclass PushToHubCallback(Callback):\n    def on_train_epoch_end(self, trainer, pl_module):\n        print(f\"Pushing model to the hub, epoch {trainer.current_epoch}\")\n        pl_module.model.push_to_hub(\"hoangphu7122002ai/donutExtractBarDot\",\n                                    commit_message=f\"model_Training in progress, epoch_ {trainer.current_epoch}\")\n        pl_module.processor.push_to_hub(\"hoangphu7122002ai/donutExtractBarDot\",\n                                    commit_message=f\"pro_Training in progress, epoch_ {trainer.current_epoch}\")\n\n    def on_train_end(self, trainer, pl_module):\n        print(f\"Pushing model to the hub after training\")\n        pl_module.processor.push_to_hub(\"hoangphu7122002ai/donutExtractBarDot\",\n                                    commit_message=f\"pro_Training done\")\n        pl_module.model.push_to_hub(\"hoangphu7122002ai/donutExtractBarDot\",\n                                    commit_message=f\"model_Training done\")\n\n    \ntrainer = pl.Trainer(\n        accelerator=\"gpu\",\n        devices=2,\n        max_epochs=CFG.epochs,\n        val_check_interval=CFG.val_check_interval,\n        check_val_every_n_epoch=CFG.check_val_every_n_epoch,\n        gradient_clip_val=CFG.gradient_clip_val,\n        precision=16, # if you have tensor cores (t4, v100, a100, etc.) training will be 2x faster\n        num_sanity_val_steps=5,\n        callbacks=[PushToHubCallback()], \n        logger=loggers\n)\n\ntrainer.fit(model_module, train_dataloaders=train_dataloader, val_dataloaders=val_dataloader)","metadata":{"execution":{"iopub.status.busy":"2023-05-31T17:24:28.384336Z","iopub.execute_input":"2023-05-31T17:24:28.384784Z","iopub.status.idle":"2023-05-31T18:44:27.980563Z","shell.execute_reply.started":"2023-05-31T17:24:28.384745Z","shell.execute_reply":"2023-05-31T18:44:27.977476Z"},"trusted":true},"execution_count":30,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max‚Ä¶","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3ebf5083480741f4873f01fa3599f9be"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>dot_score</td><td>‚ñÅ‚ñÖ‚ñà‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>train_loss</td><td>‚ñÖ‚ñÉ‚ñÉ‚ñÑ‚ñÜ‚ñÖ‚ñÇ‚ñÉ‚ñÜ‚ñÖ‚ñÇ‚ñÖ‚ñà‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÉ‚ñÑ‚ñÖ‚ñÑ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñà‚ñÉ‚ñÖ‚ñÉ‚ñÇ‚ñÅ‚ñÇ‚ñÉ‚ñÑ</td></tr><tr><td>trainer/global_step</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>val_score</td><td>‚ñÅ‚ñà‚ñà‚ñÑ‚ñá‚ñà‚ñà</td></tr><tr><td>vertical_bar_score</td><td>‚ñÅ‚ñà‚ñÜ‚ñÅ‚ñÖ‚ñÜ‚ñÜ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>dot_score</td><td>0.98541</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>train_loss</td><td>0.64867</td></tr><tr><td>trainer/global_step</td><td>0</td></tr><tr><td>val_score</td><td>0.68838</td></tr><tr><td>vertical_bar_score</td><td>0.43233</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">worthy-star-10</strong> at: <a href='https://wandb.ai/hp7122002/benetech/runs/n9j8w97m' target=\"_blank\">https://wandb.ai/hp7122002/benetech/runs/n9j8w97m</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20230531_091356-n9j8w97m/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.15.3 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.13.10"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>./wandb/run-20230531_172432-uiaij7yx</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/hp7122002/benetech/runs/uiaij7yx' target=\"_blank\">easy-dream-11</a></strong> to <a href='https://wandb.ai/hp7122002/benetech' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/hp7122002/benetech' target=\"_blank\">https://wandb.ai/hp7122002/benetech</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/hp7122002/benetech/runs/uiaij7yx' target=\"_blank\">https://wandb.ai/hp7122002/benetech/runs/uiaij7yx</a>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:209: UserWarning: num_workers>0, persistent_workers=False, and strategy=ddp_spawn may result in data loading bottlenecks. Consider setting persistent_workers=True (this is a limitation of Python .spawn() and PyTorch)\n  \"num_workers>0, persistent_workers=False, and strategy=ddp_spawn\"\n","output_type":"stream"},{"name":"stdout","text":"\n {'val_score': 0.5550743035479223, 'vertical_bar_score': 0.5550743035479223}\n\n {'val_score': 0.303801120103538, 'vertical_bar_score': 0.30380112010353805}\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:540: PossibleUserWarning: It is recommended to use `self.log('val_score', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n  category=PossibleUserWarning,\n/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:540: PossibleUserWarning: It is recommended to use `self.log('vertical_bar_score', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n  category=PossibleUserWarning,\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"93a64cc6d16949fbac6cd901f3464240"}},"metadata":{}},{"name":"stderr","text":"[W reducer.cpp:1298] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n[W reducer.cpp:1298] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validation: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"\n {'val_score': 0.6713945627987514, 'dot_score': 0.9897876438301748, 'vertical_bar_score': 0.39691776880614504}\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:540: PossibleUserWarning: It is recommended to use `self.log('dot_score', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n  category=PossibleUserWarning,\n","output_type":"stream"},{"name":"stdout","text":"Pushing model to the hub, epoch 0\n\n {'val_score': 0.7463898842178386, 'dot_score': 0.9895759439266913, 'vertical_bar_score': 0.5367467292964135}\nPushing model to the hub, epoch 0\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/809M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d240fcf255e8418da1949e78895bc1ef"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Upload 1 LFS files:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"13b28b26cdae4a21a70fda9b4836c437"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/809M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0b857f592b644ceabdba5ae95666b14f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Upload 1 LFS files:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fa7602530cf7431e9c63c55309384903"}},"metadata":{}},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mProcessRaisedException\u001b[0m                    Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_294/800547340.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     45\u001b[0m )\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataloaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    607\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lightning_module\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    608\u001b[0m         call._call_and_handle_interrupt(\n\u001b[0;32m--> 609\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_impl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataloaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatamodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mckpt_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    610\u001b[0m         )\n\u001b[1;32m    611\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/call.py\u001b[0m in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlauncher\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlauncher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlaunch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainer_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mtrainer_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pytorch_lightning/strategies/launchers/multiprocessing.py\u001b[0m in \u001b[0;36mlaunch\u001b[0;34m(self, function, trainer, *args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m             \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprocess_args\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m             \u001b[0mnprocs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_strategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_processes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0mstart_method\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_method\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         )\n\u001b[1;32m    119\u001b[0m         \u001b[0mworker_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/multiprocessing/spawn.py\u001b[0m in \u001b[0;36mstart_processes\u001b[0;34m(fn, args, nprocs, join, daemon, start_method)\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0;31m# Loop on join until it returns True or raises an exception.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 198\u001b[0;31m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m         \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/multiprocessing/spawn.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    158\u001b[0m         \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\\n\\n-- Process %d terminated with the following error:\\n\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0merror_index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[0mmsg\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0moriginal_trace\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mProcessRaisedException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfailed_process\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mProcessRaisedException\u001b[0m: \n\n-- Process 0 terminated with the following error:\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.7/site-packages/huggingface_hub/utils/_errors.py\", line 259, in hf_raise_for_status\n    response.raise_for_status()\n  File \"/opt/conda/lib/python3.7/site-packages/requests/models.py\", line 1021, in raise_for_status\n    raise HTTPError(http_error_msg, response=self)\nrequests.exceptions.HTTPError: 412 Client Error: Precondition Failed for url: https://huggingface.co/api/models/hoangphu7122002ai/donutExtractBarDot/commit/main\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.7/site-packages/torch/multiprocessing/spawn.py\", line 69, in _wrap\n    fn(i, *args)\n  File \"/opt/conda/lib/python3.7/site-packages/pytorch_lightning/strategies/launchers/multiprocessing.py\", line 139, in _wrapping_function\n    results = function(*args, **kwargs)\n  File \"/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\", line 650, in _fit_impl\n    self._run(model, ckpt_path=self.ckpt_path)\n  File \"/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\", line 1112, in _run\n    results = self._run_stage()\n  File \"/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\", line 1191, in _run_stage\n    self._run_train()\n  File \"/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\", line 1214, in _run_train\n    self.fit_loop.run()\n  File \"/opt/conda/lib/python3.7/site-packages/pytorch_lightning/loops/loop.py\", line 200, in run\n    self.on_advance_end()\n  File \"/opt/conda/lib/python3.7/site-packages/pytorch_lightning/loops/fit_loop.py\", line 295, in on_advance_end\n    self.trainer._call_callback_hooks(\"on_train_epoch_end\")\n  File \"/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\", line 1394, in _call_callback_hooks\n    fn(self, self.lightning_module, *args, **kwargs)\n  File \"/tmp/ipykernel_294/800547340.py\", line 24, in on_train_epoch_end\n    commit_message=f\"pro_Training in progress, epoch_ {trainer.current_epoch}\")\n  File \"/opt/conda/lib/python3.7/site-packages/transformers/utils/hub.py\", line 798, in push_to_hub\n    create_pr=create_pr,\n  File \"/opt/conda/lib/python3.7/site-packages/transformers/utils/hub.py\", line 709, in _upload_modified_files\n    repo_id=repo_id, operations=operations, commit_message=commit_message, token=token, create_pr=create_pr\n  File \"/opt/conda/lib/python3.7/site-packages/huggingface_hub/utils/_validators.py\", line 120, in _inner_fn\n    return fn(*args, **kwargs)\n  File \"/opt/conda/lib/python3.7/site-packages/huggingface_hub/hf_api.py\", line 2548, in create_commit\n    hf_raise_for_status(commit_resp, endpoint_name=\"commit\")\n  File \"/opt/conda/lib/python3.7/site-packages/huggingface_hub/utils/_errors.py\", line 301, in hf_raise_for_status\n    raise HfHubHTTPError(str(e), response=response) from e\nhuggingface_hub.utils._errors.HfHubHTTPError: 412 Client Error: Precondition Failed for url: https://huggingface.co/api/models/hoangphu7122002ai/donutExtractBarDot/commit/main (Request ID: Root=1-64779588-75cf7eb557a2aca304c02249)\n\nA commit has happened since. Please refresh and try again.\n"],"ename":"ProcessRaisedException","evalue":"\n\n-- Process 0 terminated with the following error:\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.7/site-packages/huggingface_hub/utils/_errors.py\", line 259, in hf_raise_for_status\n    response.raise_for_status()\n  File \"/opt/conda/lib/python3.7/site-packages/requests/models.py\", line 1021, in raise_for_status\n    raise HTTPError(http_error_msg, response=self)\nrequests.exceptions.HTTPError: 412 Client Error: Precondition Failed for url: https://huggingface.co/api/models/hoangphu7122002ai/donutExtractBarDot/commit/main\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.7/site-packages/torch/multiprocessing/spawn.py\", line 69, in _wrap\n    fn(i, *args)\n  File \"/opt/conda/lib/python3.7/site-packages/pytorch_lightning/strategies/launchers/multiprocessing.py\", line 139, in _wrapping_function\n    results = function(*args, **kwargs)\n  File \"/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\", line 650, in _fit_impl\n    self._run(model, ckpt_path=self.ckpt_path)\n  File \"/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\", line 1112, in _run\n    results = self._run_stage()\n  File \"/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\", line 1191, in _run_stage\n    self._run_train()\n  File \"/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\", line 1214, in _run_train\n    self.fit_loop.run()\n  File \"/opt/conda/lib/python3.7/site-packages/pytorch_lightning/loops/loop.py\", line 200, in run\n    self.on_advance_end()\n  File \"/opt/conda/lib/python3.7/site-packages/pytorch_lightning/loops/fit_loop.py\", line 295, in on_advance_end\n    self.trainer._call_callback_hooks(\"on_train_epoch_end\")\n  File \"/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\", line 1394, in _call_callback_hooks\n    fn(self, self.lightning_module, *args, **kwargs)\n  File \"/tmp/ipykernel_294/800547340.py\", line 24, in on_train_epoch_end\n    commit_message=f\"pro_Training in progress, epoch_ {trainer.current_epoch}\")\n  File \"/opt/conda/lib/python3.7/site-packages/transformers/utils/hub.py\", line 798, in push_to_hub\n    create_pr=create_pr,\n  File \"/opt/conda/lib/python3.7/site-packages/transformers/utils/hub.py\", line 709, in _upload_modified_files\n    repo_id=repo_id, operations=operations, commit_message=commit_message, token=token, create_pr=create_pr\n  File \"/opt/conda/lib/python3.7/site-packages/huggingface_hub/utils/_validators.py\", line 120, in _inner_fn\n    return fn(*args, **kwargs)\n  File \"/opt/conda/lib/python3.7/site-packages/huggingface_hub/hf_api.py\", line 2548, in create_commit\n    hf_raise_for_status(commit_resp, endpoint_name=\"commit\")\n  File \"/opt/conda/lib/python3.7/site-packages/huggingface_hub/utils/_errors.py\", line 301, in hf_raise_for_status\n    raise HfHubHTTPError(str(e), response=response) from e\nhuggingface_hub.utils._errors.HfHubHTTPError: 412 Client Error: Precondition Failed for url: https://huggingface.co/api/models/hoangphu7122002ai/donutExtractBarDot/commit/main (Request ID: Root=1-64779588-75cf7eb557a2aca304c02249)\n\nA commit has happened since. Please refresh and try again.\n","output_type":"error"}]},{"cell_type":"markdown","source":"I interrupted the run because I was impatient","metadata":{}},{"cell_type":"code","source":"trainer.validate(model_module, dataloaders=val_dataloader)","metadata":{"execution":{"iopub.status.busy":"2023-05-31T18:44:42.317588Z","iopub.execute_input":"2023-05-31T18:44:42.318871Z","iopub.status.idle":"2023-05-31T18:55:07.983207Z","shell.execute_reply.started":"2023-05-31T18:44:42.318793Z","shell.execute_reply":"2023-05-31T18:55:07.980835Z"},"trusted":true},"execution_count":31,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:320: PossibleUserWarning: Using `DistributedSampler` with the dataloaders. During `trainer.validate()`, it is recommended to use `Trainer(devices=1, num_nodes=1)` to ensure each sample/batch gets evaluated exactly once. Otherwise, multi-device settings use `DistributedSampler` that replicates some samples to make sure all devices have same batch size in case of uneven inputs.\n  category=PossibleUserWarning,\n/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:209: UserWarning: num_workers>0, persistent_workers=False, and strategy=ddp_spawn may result in data loading bottlenecks. Consider setting persistent_workers=True (this is a limitation of Python .spawn() and PyTorch)\n  \"num_workers>0, persistent_workers=False, and strategy=ddp_spawn\"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validation: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f5de85014d7f45838a17c60743d9eae3"}},"metadata":{}},{"name":"stdout","text":"\n {'val_score': 0.6799440816939416, 'dot_score': 0.98940639502875, 'vertical_bar_score': 0.413166225370831}\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:540: PossibleUserWarning: It is recommended to use `self.log('val_score', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n  category=PossibleUserWarning,\n/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:540: PossibleUserWarning: It is recommended to use `self.log('dot_score', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n  category=PossibleUserWarning,\n/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:540: PossibleUserWarning: It is recommended to use `self.log('vertical_bar_score', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n  category=PossibleUserWarning,\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì\n‚îÉ\u001b[1m \u001b[0m\u001b[1m     Validate metric     \u001b[0m\u001b[1m \u001b[0m‚îÉ\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m‚îÉ\n‚î°‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î©\n‚îÇ\u001b[36m \u001b[0m\u001b[36m        dot_score        \u001b[0m\u001b[36m \u001b[0m‚îÇ\u001b[35m \u001b[0m\u001b[35m    0.989406406879425    \u001b[0m\u001b[35m \u001b[0m‚îÇ\n‚îÇ\u001b[36m \u001b[0m\u001b[36m        val_score        \u001b[0m\u001b[36m \u001b[0m‚îÇ\u001b[35m \u001b[0m\u001b[35m   0.6799440816939416    \u001b[0m\u001b[35m \u001b[0m‚îÇ\n‚îÇ\u001b[36m \u001b[0m\u001b[36m   vertical_bar_score    \u001b[0m\u001b[36m \u001b[0m‚îÇ\u001b[35m \u001b[0m\u001b[35m   0.41316622495651245   \u001b[0m\u001b[35m \u001b[0m‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì\n‚îÉ<span style=\"font-weight: bold\">      Validate metric      </span>‚îÉ<span style=\"font-weight: bold\">       DataLoader 0        </span>‚îÉ\n‚î°‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î©\n‚îÇ<span style=\"color: #008080; text-decoration-color: #008080\">         dot_score         </span>‚îÇ<span style=\"color: #800080; text-decoration-color: #800080\">     0.989406406879425     </span>‚îÇ\n‚îÇ<span style=\"color: #008080; text-decoration-color: #008080\">         val_score         </span>‚îÇ<span style=\"color: #800080; text-decoration-color: #800080\">    0.6799440816939416     </span>‚îÇ\n‚îÇ<span style=\"color: #008080; text-decoration-color: #008080\">    vertical_bar_score     </span>‚îÇ<span style=\"color: #800080; text-decoration-color: #800080\">    0.41316622495651245    </span>‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n</pre>\n"},"metadata":{}},{"name":"stdout","text":"\n {'val_score': 0.7726922589251383, 'dot_score': 0.9883569112864365, 'vertical_bar_score': 0.5867744551653983}\n","output_type":"stream"},{"execution_count":31,"output_type":"execute_result","data":{"text/plain":"[{'val_score': 0.6799440816939416,\n  'dot_score': 0.989406406879425,\n  'vertical_bar_score': 0.41316622495651245}]"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_module.processor.push_to_hub(\"hoangphu7122002ai/donutExtractBarDot\",\n                                    commit_message=f\"Training done\")\nmodel_module.model.push_to_hub(\"hoangphu7122002ai/donutExtractBarDot\",\n                                    commit_message=f\"Training done\")","metadata":{"execution":{"iopub.status.busy":"2023-05-25T05:23:54.851381Z","iopub.execute_input":"2023-05-25T05:23:54.851818Z","iopub.status.idle":"2023-05-25T05:24:20.799828Z","shell.execute_reply.started":"2023-05-25T05:23:54.851756Z","shell.execute_reply":"2023-05-25T05:24:20.798496Z"},"trusted":true},"execution_count":29,"outputs":[{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/809M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a96f12042c9e40899ade75b86f66a66d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Upload 1 LFS files:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7195af152d9b42878ec3df3a2caa0043"}},"metadata":{}},{"execution_count":29,"output_type":"execute_result","data":{"text/plain":"CommitInfo(commit_url='https://huggingface.co/hoangphu7122002ai/donutExtract/commit/b86194a7316ef8618bdf005ffa4ff1a539fa7e70', commit_message='Training done', commit_description='', oid='b86194a7316ef8618bdf005ffa4ff1a539fa7e70', pr_url=None, pr_revision=None, pr_num=None)"},"metadata":{}}]},{"cell_type":"code","source":"# Could also use the checkpoint callback\n# model_module.model.save_pretrained(CFG.output_path)\n# model_module.processor.save_pretrained(CFG.output_path)","metadata":{"execution":{"iopub.status.busy":"2023-05-25T05:07:43.803539Z","iopub.status.idle":"2023-05-25T05:07:43.804394Z","shell.execute_reply.started":"2023-05-25T05:07:43.804110Z","shell.execute_reply":"2023-05-25T05:07:43.804137Z"},"trusted":true},"execution_count":null,"outputs":[]}]}